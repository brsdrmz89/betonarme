# -*- coding: utf-8 -*-
"""
PostgreSQL RAG System for Betonarme Ä°ÅŸÃ§ilik ModÃ¼lÃ¼
pgvector olmadan basit implementasyon
"""

import os
import json
import logging
import psycopg2
import psycopg2.extras
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, date, timedelta
import pandas as pd
from dataclasses import dataclass

# Logging ayarla
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RAGConfig:
    """RAG sistem konfigÃ¼rasyonu"""
    db_host: str = "localhost"
    db_port: int = 5432
    db_name: str = "betonarme_rag"
    db_user: str = "postgres"
    db_password: str = "1905"
    
    # Retriever ayarlarÄ±
    default_top_k: int = 8
    score_threshold: float = 0.78
    min_sources: int = 2
    
    # GÃ¼venlik ayarlarÄ±
    enable_audit_log: bool = True
    enable_source_diversification: bool = True
    enable_score_filtering: bool = True

class PostgreSQLRAGSystem:
    """PostgreSQL tabanlÄ± RAG sistemi"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.connection = None
        self._connect()
    
    def _connect(self):
        """VeritabanÄ±na baÄŸlan"""
        try:
            self.connection = psycopg2.connect(
                host=self.config.db_host,
                port=self.config.db_port,
                database=self.config.db_name,
                user=self.config.db_user,
                password=self.config.db_password
            )
            self.connection.autocommit = True
            logger.info("âœ… PostgreSQL baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±")
        except Exception as e:
            logger.error(f"âŒ PostgreSQL baÄŸlantÄ± hatasÄ±: {e}")
            raise
    
    def add_document(self, source: str, country: str, doc_type: str, 
                    title: str, lang: str, content: str, **kwargs) -> int:
        """DokÃ¼man ekle"""
        cursor = self.connection.cursor()
        
        # DokÃ¼manÄ± ekle
        cursor.execute("""
        INSERT INTO documents (source, country, doc_type, title, lang, content)
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING id
        """, (source, country, doc_type, title, lang, content))
        
        doc_id = cursor.fetchone()[0]
        
        # Ä°Ã§eriÄŸi chunk'lara bÃ¶l
        chunks = self._chunk_text(content, title)
        
        # Chunk'larÄ± ekle
        for chunk in chunks:
            self._add_chunk(doc_id, chunk)
        
        logger.info(f"âœ… DokÃ¼man eklendi: {title} ({len(chunks)} chunk)")
        return doc_id
    
    def _chunk_text(self, text: str, title: str) -> List[Dict]:
        """Metni chunk'lara bÃ¶l"""
        max_tokens = 1000
        words = text.split()
        chunks = []
        
        current_chunk = []
        current_tokens = 0
        
        for word in words:
            current_chunk.append(word)
            current_tokens += len(word.split()) + 1
            
            if current_tokens >= max_tokens:
                chunk_text = " ".join(current_chunk)
                chunks.append({
                    'text': chunk_text,
                    'tokens': current_tokens,
                    'heading': title,
                    'section_path': title
                })
                current_chunk = []
                current_tokens = 0
        
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'tokens': current_tokens,
                'heading': title,
                'section_path': title
            })
        
        return chunks
    
    def _add_chunk(self, document_id: int, chunk: Dict):
        """Chunk ekle"""
        work_types = self._extract_work_types(chunk['text'])
        norm_codes = self._extract_norm_codes(chunk['text'])
        unit = self._extract_unit(chunk['text'])
        
        cursor = self.connection.cursor()
        cursor.execute("""
        INSERT INTO chunks (document_id, section_path, heading, text, tokens, 
                           work_types, norm_codes, unit, locale)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            document_id,
            chunk['section_path'],
            chunk['heading'],
            chunk['text'],
            chunk['tokens'],
            work_types,
            norm_codes,
            unit,
            'tr'
        ))
    
    def _extract_work_types(self, text: str) -> List[str]:
        """Ä°ÅŸ tiplerini Ã§Ä±kar"""
        work_types = []
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['donatÄ±', 'Ğ°Ñ€Ğ¼Ğ°Ñ‚ÑƒÑ€Ğ°', 'rebar']):
            work_types.append('rebar')
        if any(word in text_lower for word in ['kalÄ±p', 'Ğ¾Ğ¿Ğ°Ğ»ÑƒĞ±ĞºĞ°', 'formwork']):
            work_types.append('formwork')
        if any(word in text_lower for word in ['beton', 'Ğ±ĞµÑ‚Ğ¾Ğ½', 'concrete']):
            work_types.append('concrete')
        
        return work_types
    
    def _extract_norm_codes(self, text: str) -> List[str]:
        """Norm kodlarÄ±nÄ± Ã§Ä±kar"""
        import re
        norm_codes = []
        
        fer_pattern = r'FER[-\s]?\d{2}[-\s]?\d{3}'
        fer_matches = re.findall(fer_pattern, text, re.IGNORECASE)
        norm_codes.extend(fer_matches)
        
        poz_pattern = r'Poz[-\s]?\d{3}'
        poz_matches = re.findall(poz_pattern, text, re.IGNORECASE)
        norm_codes.extend(poz_matches)
        
        return norm_codes
    
    def _extract_unit(self, text: str) -> Optional[str]:
        """Birim Ã§Ä±kar"""
        text_lower = text.lower()
        
        if 'kg' in text_lower:
            return 'kg'
        elif 'm2' in text_lower or 'mÂ²' in text_lower:
            return 'm2'
        elif 'm3' in text_lower or 'mÂ³' in text_lower:
            return 'm3'
        elif 'saat' in text_lower:
            return 'h'
        
        return None
    
    def search(self, query: str, locales: List[str] = None, 
              work_types: List[str] = None, top_k: int = None) -> List[Dict]:
        """Basit metin arama"""
        if not top_k:
            top_k = self.config.default_top_k
        
        if not locales:
            locales = ['tr', 'ru', 'en']
        
        cursor = self.connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Basit LIKE arama
        search_query = """
        SELECT c.id, c.document_id, c.section_path, c.heading, c.text,
               c.work_types, c.norm_codes, c.unit, c.locale,
               d.source, d.country, d.title as doc_title
        FROM chunks c
        JOIN documents d ON c.document_id = d.id
        WHERE c.text ILIKE %s AND c.locale = ANY(%s)
        ORDER BY 
            CASE 
                WHEN c.text ILIKE %s THEN 1
                WHEN c.heading ILIKE %s THEN 2
                ELSE 3
            END,
            c.created_at DESC
        LIMIT %s
        """
        
        query_pattern = f'%{query}%'
        params = [query_pattern, locales, query_pattern, query_pattern, top_k]
        
        cursor.execute(search_query, params)
        results = cursor.fetchall()
        
        # SonuÃ§larÄ± iÅŸle
        processed_results = []
        for row in results:
            processed_results.append({
                'id': row['id'],
                'document_id': row['document_id'],
                'section_path': row['section_path'],
                'heading': row['heading'],
                'text': row['text'],
                'work_types': row['work_types'] or [],
                'norm_codes': row['norm_codes'] or [],
                'unit': row['unit'],
                'locale': row['locale'],
                'score': 0.8,  # Sabit skor
                'source': row['source'],
                'country': row['country'],
                'doc_title': row['doc_title']
            })
        
        # GÃ¼venlik katmanÄ± uygula
        filtered_results = self._apply_security_layer(query, processed_results)
        
        # Audit log
        if self.config.enable_audit_log:
            self._log_retrieval(query, filtered_results)
        
        return filtered_results
    
    def _apply_security_layer(self, query: str, results: List[Dict]) -> List[Dict]:
        """GÃ¼venlik katmanÄ± uygula"""
        
        # Kaynak Ã§eÅŸitlendirmesi
        if self.config.enable_source_diversification:
            results = self._diversify_sources(results)
        
        # Minimum kaynak kontrolÃ¼
        if len(results) < self.config.min_sources:
            logger.warning(f"Insufficient context: only {len(results)} sources found")
        
        return results
    
    def _diversify_sources(self, results: List[Dict]) -> List[Dict]:
        """Kaynak Ã§eÅŸitlendirmesi uygula"""
        source_counts = {}
        diversified = []
        
        for result in results:
            source = result['source']
            if source_counts.get(source, 0) < 2:  # Her kaynaktan max 2 chunk
                diversified.append(result)
                source_counts[source] = source_counts.get(source, 0) + 1
        
        return diversified
    
    def _log_retrieval(self, query: str, results: List[Dict]):
        """Retrieval iÅŸlemini logla"""
        cursor = self.connection.cursor()
        
        cursor.execute("""
        INSERT INTO retrieval_logs (query, top_k, chunk_ids, scores, accepted)
        VALUES (%s, %s, %s, %s, %s)
        """, (
            query,
            len(results),
            [r['id'] for r in results],
            [r['score'] for r in results],
            len(results) >= self.config.min_sources
        ))
    
    def calculate_labor_hours(self, wbs_key: str, qty: float, unit: str, 
                            locale: str = 'tr') -> float:
        """Ä°ÅŸÃ§ilik saati hesapla"""
        cursor = self.connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Uygun normu bul
        cursor.execute("""
        SELECT norm_lh_per_u, conditions_json
        FROM norms
        WHERE work_item_key = %s AND unit = %s AND locale = %s
        ORDER BY 
            CASE source 
                WHEN 'Internal' THEN 1
                WHEN 'Poz' THEN 2
                WHEN 'FER' THEN 3
                ELSE 4
            END,
            updated_at DESC
        LIMIT 1
        """, (wbs_key, unit, locale))
        
        result = cursor.fetchone()
        if not result:
            logger.warning(f"Norm bulunamadÄ±: {wbs_key}, {unit}")
            return 0.0
        
        norm_lh_per_u = float(result['norm_lh_per_u'])
        conditions = result['conditions_json'] or {}
        
        # KoÅŸul Ã§arpanlarÄ±nÄ± uygula
        adjustment_factors = self._calculate_adjustment_factors(conditions)
        effective_norm = norm_lh_per_u * adjustment_factors
        
        return qty * effective_norm
    
    def _calculate_adjustment_factors(self, conditions: Dict) -> float:
        """KoÅŸul Ã§arpanlarÄ±nÄ± hesapla"""
        factors = 1.0
        
        if conditions.get('height') == '>3m':
            factors *= 1.15
        if conditions.get('weather') == 'cold':
            factors *= 1.20
        if conditions.get('complexity') == 'high':
            factors *= 1.25
        
        return factors
    
    def add_sample_data(self):
        """Ã–rnek veri ekle"""
        logger.info("ğŸ“Š Ã–rnek veri ekleniyor...")
        
        # Revit quantities
        cursor = self.connection.cursor()
        revit_data = [
            ('model_001', 'elem_001', 'Structural Framing', 'rebar', 'REBAR.BEAM', 1500.0, 'kg'),
            ('model_001', 'elem_002', 'Structural Framing', 'formwork', 'FORM.BEAM', 25.0, 'm2'),
            ('model_001', 'elem_003', 'Structural Framing', 'concrete', 'CONC.BEAM', 2.5, 'm3'),
        ]
        
        for revit in revit_data:
            cursor.execute("""
            INSERT INTO revit_quantities (model_id, element_id, category, class_inf, wbs_key, qty, unit)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (model_id, element_id) DO NOTHING
            """, revit)
        
        # Site observations
        today = date.today()
        site_data = [
            (today, 'day', 'crew_001', 'REBAR.BEAM', 1500.0, 'kg', 180.0),
            (today, 'day', 'crew_002', 'FORM.BEAM', 25.0, 'm2', 20.0),
            (today, 'day', 'crew_003', 'CONC.BEAM', 2.5, 'm3', 1.25),
        ]
        
        for site in site_data:
            cursor.execute("""
            INSERT INTO site_observations (work_date, shift, crew_id, wbs_key, qty, unit, labor_hours)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, site)
        
        logger.info("âœ… Ã–rnek veri eklendi")
    
    def export_reports(self, start_date: date, end_date: date, 
                      output_dir: str = ".") -> Dict[str, str]:
        """RaporlarÄ± ihraÃ§ et"""
        logger.info("ğŸ“Š Raporlar ihraÃ§ ediliyor...")
        
        cursor = self.connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Variance summary
        cursor.execute("""
        SELECT 
            rq.wbs_key,
            rq.qty,
            rq.unit,
            COALESCE(SUM(so.labor_hours), 0) as actual_hours
        FROM revit_quantities rq
        LEFT JOIN site_observations so ON rq.wbs_key = so.wbs_key 
            AND so.work_date BETWEEN %s AND %s
        GROUP BY rq.wbs_key, rq.qty, rq.unit
        """, (start_date, end_date))
        
        results = cursor.fetchall()
        
        # CSV oluÅŸtur
        variance_path = os.path.join(output_dir, "variance_summary.csv")
        with open(variance_path, 'w', encoding='utf-8-sig') as f:
            f.write("period;wbs_key;qty;unit;LH_theo;LH_actual;delta;delta_%;productivity\n")
            
            for row in results:
                wbs_key = row['wbs_key']
                qty = float(row['qty'])
                unit = row['unit']
                actual_hours = float(row['actual_hours'])
                
                # Teorik saat hesapla
                theoretical_hours = self.calculate_labor_hours(wbs_key, qty, unit)
                
                # Sapma hesapla
                delta = actual_hours - theoretical_hours
                delta_percent = (delta / theoretical_hours * 100) if theoretical_hours > 0 else 0
                productivity = qty / actual_hours if actual_hours > 0 else 0
                
                period = f"{start_date} - {end_date}"
                f.write(f"{period};{wbs_key};{qty};{unit};{theoretical_hours:.2f};{actual_hours:.2f};{delta:.2f};{delta_percent:.2f};{productivity:.2f}\n")
        
        logger.info(f"âœ… Rapor ihraÃ§ edildi: {variance_path}")
        return {'variance_summary': variance_path}
    
    def close(self):
        """VeritabanÄ±nÄ± kapat"""
        if self.connection:
            self.connection.close()
            logger.info("âœ… PostgreSQL baÄŸlantÄ±sÄ± kapatÄ±ldÄ±")

def demo_postgresql_rag():
    """PostgreSQL RAG demo"""
    print("ğŸ¯ PostgreSQL RAG System Demo")
    print("=" * 50)
    
    try:
        # KonfigÃ¼rasyon
        config = RAGConfig()
        
        # Sistem baÅŸlat
        rag_system = PostgreSQLRAGSystem(config)
        
        # Ã–rnek veri ekle
        rag_system.add_sample_data()
        
        # DokÃ¼man ekle
        print("\nğŸ“„ DokÃ¼man ekleniyor...")
        doc_id = rag_system.add_document(
            source="FER",
            country="RU",
            doc_type="norm",
            title="FER-06 Betonarme Ä°ÅŸleri",
            lang="ru",
            content="""
            FER-06-001: DonatÄ± baÄŸlama iÅŸleri
            Birim: kg
            Norm: 0.15 adam-saat/kg
            
            FER-06-002: KalÄ±p kurulumu
            Birim: m2
            Norm: 0.8 adam-saat/m2
            
            FER-06-003: Beton dÃ¶kÃ¼mÃ¼
            Birim: m3
            Norm: 0.5 adam-saat/m3
            """
        )
        print(f"âœ… DokÃ¼man eklendi (ID: {doc_id})")
        
        # Arama testi
        print("\nğŸ” Arama testi...")
        results = rag_system.search("donatÄ± baÄŸlama", locales=['tr', 'ru'])
        print(f"âœ… {len(results)} sonuÃ§ bulundu")
        
        for i, result in enumerate(results[:3]):
            print(f"\nğŸ“‹ SonuÃ§ {i+1}:")
            print(f"   Skor: {result['score']:.3f}")
            print(f"   Kaynak: {result['source']}")
            print(f"   Metin: {result['text'][:100]}...")
        
        # Hesaplama testi
        print("\nğŸ§® Hesaplama testi...")
        theoretical_hours = rag_system.calculate_labor_hours('REBAR.BEAM', 1500.0, 'kg')
        print(f"âœ… Teorik adam-saat: {theoretical_hours:.2f}")
        
        # Rapor ihracÄ±
        print("\nğŸ“Š Rapor ihracÄ±...")
        end_date = date.today()
        start_date = end_date - timedelta(days=7)
        
        reports = rag_system.export_reports(start_date, end_date)
        print(f"âœ… Raporlar: {list(reports.keys())}")
        
        # Sistemi kapat
        rag_system.close()
        
        print("\nğŸ‰ PostgreSQL RAG Demo baÅŸarÄ±yla tamamlandÄ±!")
        print("\nğŸ“ OluÅŸturulan dosyalar:")
        print("- variance_summary.csv (rapor)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Demo hatasÄ±: {e}")
        return False

if __name__ == "__main__":
    success = demo_postgresql_rag()
    if success:
        print("\nâœ… PostgreSQL RAG sistemi Ã§alÄ±ÅŸÄ±yor!")
    else:
        print("\nâŒ PostgreSQL RAG sistemi Ã§alÄ±ÅŸmÄ±yor.")

